<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[机器学习概述]]></title>
    <url>%2F2017%2F08%2F30%2FmachineLearning%2F</url>
    <content type="text"><![CDATA[机器学习: 从数据中学到知识。 Computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P improves with experience E. from Tom Mitchell,1998. T: 任务； P: performance measure E: experience经验，实验 基于E的不断学习，通过P的指标得出在T上的表现得到改进，我们就说程序通过经验E来学习该任务。 如预计房价： 最简单的方法就是用一根直线(也就是一次方程)来预测，用二次函数能够得到更好的结果： 这种学习称为 监督学习 supervised learning: 预先有正确的结果，可以用来指导，检验算法的正确性。 监督学习 supervised learning 1 回归regression ： 连续值预测 2 分类classification： 离散值预测，如何根据特征将对象分到不同的分类中(赋予不同的离散值) - 上面的例子只用到两个特征 features，更多的特征处理可能需要其他算法，如SVM。 非监督学习 unsupervised learning 特点：训练数据只有特征，没有结果：发现这些数据是否可以分为不同的组,应用包括： 聚类问题Clustering 声音辨别：将不同人的声音分离，使用双麦克，位置的远近，角度会造成声音的大小等不同。使用svd一行代码就能解决。 References 吴恩达，《Machine learning》，Coursera.]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为什么梯度反方向下降最快？]]></title>
    <url>%2F2017%2F08%2F20%2FwhyDescent%2F</url>
    <content type="text"><![CDATA[梯度下降算法中，为什么沿着梯度反方向是下降最快呢？用一种直观的方法”证明”一下。考虑两个features的情况。首先用泰勒一阶展开，问题转换为在点(a,b)附近（我们可以限制在以点为圆心的圆内）哪个方向可以使得函数值最小？ 转换成一个更好理解的公式：看一下公式的后两项，就是两个向量的点积，$[u,v]^ \top$是已知的，要使点积值最小（这里负值是最小的了），当然是和$[u,v]^ \top$方向相反，并且尽可能远的向量了： 参考 李宏毅，《machine learning》]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值计算需要注意的问题]]></title>
    <url>%2F2017%2F08%2F19%2FnumCalculate01%2F</url>
    <content type="text"><![CDATA[数值计算中，需要考虑计算机精度带来的一些问题。 Overflow and Underflow上溢和下溢数字计算机无法对实数进行”精确”表示,总会引入些误差（大部分是舍入误差rounding errors）。当这些误差被复合累积操作时，会造成一些问题。 误差的下溢 ： 接近零的的数被四舍五入为零。很多时候等于零与接近零的表现质的不同。上溢：当大量级的数被近似为$\infty, -\infty$,进一步计算会导致非数字。例如softmax函数，经常用于预测与multinoulli 分布相关联的概率，定义为： 考虑所有$x$都等于$c$，那么softmax的理论值为$\frac{1}{n}$。但是在数值计算中，当$c$量级很大时，可能求不到： If $c$ is very negative, $\exp (c)$会underflow：造成分母为0。 $c$是一个很大数，造成$\exp (c)$overflow； 我们可以通过计算$\mathrm{softmax} (\boldsymbol{z})$来避免这些问题，其中 $\boldsymbol{z = x} - \max _i x_i$ : 减去$\max _i x_i$导致exp最大参数为0；不会overflow； 同样的原因，分布至少有一个值为1，排除分母underflow变成被0除的可能。 另外分子的underflow可导致整体结果为零。若在计算$log (softmax(x))$ 函数中，先计算softmax，再代入计算log有可能出现负无穷结果。所以我们要实现一个单独的函数以数值稳定的方式来计算。 所以计算时一定要考虑数值不稳定的问题，必须设计为最小化舍入误差的积累。 病态条件数Poor Conditioning条件数conditioning是指函数相对于输入的微小变化而变化的快慢程度，而迅速变化的函数是不理想的：舍入误差会造成很大的变化。 考虑函数$f(x) = A^{-1} x$, 当$A \in \mathbb{R}^{n \times n}$具有特征分解时，其条件数为：$$\underset{i,j}{\max}~ \Bigg| \frac{\lambda_i}{ \lambda_j} \Bigg|.$$这是最大和最小特征值的模之比,当该数很大时，矩阵求逆对输入的误差特别敏感。注意这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结果。 即使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存在的误差。 在实践中，该错误将与求逆过程本身的数值误差进一步复合。]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>数值计算</tag>
        <tag>overflow &amp; underflow</tag>
        <tag>poor conditioning</tag>
      </tags>
  </entry>
</search>
