<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[sigmoid]]></title>
    <url>%2F2017%2F08%2F31%2Fsigmoid%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[6 Probabilistic Generative Model概率生成模型]]></title>
    <url>%2F2017%2F08%2F31%2Fpgm%2F</url>
    <content type="text"><![CDATA[img{display:block;margin: o auto} 在介绍洛基回归模型之前，我们先了解以下概率生成模型。贝叶斯模型给了我们发现反向概率的方法： 对于一般的分类模型，我们也可以采用类似的方法，若果$P(C_1 | \boldsymbol{x}) &gt; 0.5$ ， $ \boldsymbol{x}$属于$C_1$，否则属于$C_2$。 上面的box，所有的概率分布都很清楚，可以直接得到最终的概率。但对于一个分类来说，我们要求$P(x|C_i)$就需要知道不同分类的概率分布。以一个Pockmon游戏中宠物分类的例子，取了两个类别水类（79个）和普通类（61个）的宠物，每个宠物有6个features。 对于训练集来说，$p(c_1), p(c_2)$是常数，统计一下就可以： 对于一般的分类模型，我们用features向量$\boldsymbol{x}$来表示一个物种。剩下的就是要求$P(\boldsymbol{x}|C_i)$：也就每一个分类中，一个未知$\boldsymbol{x}$在该类出现的概率／密度函数。我们目标是就是需要根据sample数据，得到不同分类的概率密度函数$P(\boldsymbol{x}|C_i)$。假设每个分类服从高斯分布： 我们就需要根据sample 数据得到均值$\mu$, 协方差矩阵covariance matrix $\Sigma$。很自然想到使用最大似然估计得到$\mu ^, \Sigma ^$：得到最接近sample数据的高斯分布。 我们根据最大似然估计，对水类和普通类得到对应的高斯分布（以两个features为例）：对于一个未知的物种$\boldsymbol{x}$,我们就可以得到他在不同类别中的概率，可以用前面的贝叶斯做分类了：最后在测试集的结果不太好，准确率只有47%，用上全部6个features的话，准确率是64%。### 模型改进前面两个分类的covariance matrix 是在各自分类中计算得到的，结果不同。现在把他们的covariance matrix 变成一个，目的是减少模型的偏差variance，也就是降低模型复杂度，减少overfitting。计算方法也很简单，就是将原来分类的$\Sigma ^i$做一个加权平均即可。再用新的参数来做分类，准确率提高到了73%！。可以看出，分类边界变成了线性边界,分类的模型也就变成了线性模型了，后面我们再看为什么变成了线性模型。总结一下，这种模型分为3步： 概率分布的选择前面我们使用的是多维高斯分布，当然你也可以使用其他的概率分布做，如下面用一维的高斯分布，假设所有的featutes之间独立，那么这就是朴素贝叶斯分类了： 后验概率和Sigmoid函数我们将上面的贝叶斯公式做一下转变： 我们看到，我们将后验概率表示成一个sigmoid函数，而$z$是$\boldsymbol{x}$在两个分类中概率比率的ln值，将出现概率的对比映射到(0,1) 之间，很明显： 当$\boldsymbol{x}$在两类出现的概率相同时，$z = 0$，就是sigmoid函数中间的点。 当$\boldsymbol{x}$在类1中概率更大时，$z &gt; 0$就是右侧。 当$\boldsymbol{x}$在类1中概率更小时，$z &lt; 0$就是左侧。 这样我们用sigmoid表示后验概率： 计算$z$的表达式： $z$看着挺复杂的，那么把上面的$\Sigma ^1, \Sigma ^2$按照前面的做法合一做进一步简化，可以更清晰的将$z$表示为一个一维函数了： (哇哇！！逻辑回归的味道来了！)这样经过一系列计算，我们将PGM模型转换为了基于sigmoid的线性模型（当然我们是简化合并了$\Sigma$），这就能解释上面的宠物分类在$\Sigma$合并后由曲线的分类边界变成了线性的分类边界了。]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>Probabilistic-Generative-Model</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[5 分类问题]]></title>
    <url>%2F2017%2F08%2F31%2Fclassification%2F</url>
    <content type="text"><![CDATA[这节，我们来看一下分类问题： 哪些邮件是垃圾邮件/正常邮件？ 我们先说二元分类： 因变量(dependant variable)可能属于的两个类： negative class 负向类 positive class 正向类 则因变量：$y \in {0,1}$,其中0表示负向类，1表示正向类。 分类问题建模线性回归建模 我们先尝试用线性回归的方法来建模： 取阈值为0.5，貌似预测的结果不错；但若训练集中有一个更大的数据(一个很靠右的训练点)，这样直线拟合的更趋缓一些： 那么我们再使用0.5做阈值就不行了。 这是因为线性回归模型预测的值可以超越[0,1]的范围，并不适合解决这样的问题。 所以我们引入一个新的模型： 逻辑回归，该模型的输出范围始终在0，1之间。]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>classification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[4 多项式回归]]></title>
    <url>%2F2017%2F08%2F31%2FpolynomialRegression%2F</url>
    <content type="text"><![CDATA[前面提到的线性回归，但实际上很多问题是不能用一根直线来区分的。有时需要曲线来拟合我们的数据，那么这是我们就可以考虑多项式回归。比如一个二次方模型quadratic model：$$h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 $$ 或 三次方模型 cube function：$$h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \theta_3 x^3 $$ 拟合效果如下： 上例中，只有一个feature即size，对该参数实行多项式计算；注意，quadratic model最后会下行，而cube function 最后会上行。 其实还可以用另一个模型，拟合的更好： $h_{\theta}(x) = \theta_0 + \theta_1 x + \theta_2 x^{\frac{1}{2}} $. 通常我们要观察数据( 图形是一个直观的观察方式 )，然后才能决定用什么样的模型。 这里面也要考虑特征feature的选择:洞察问题，选择最佳的features。 有时还需要对特征进行组合：若原始feature是房屋的长，宽，这显然不是很好的特征，把他们组合新生成一个特征：面积。 注意：我们采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。 正规方程normal equation 某些情况，能够直接求解”最佳”$\theta$ 前面我们一直用梯度下降法找到使cost function 最小化的参数。 对于线性回归等(凸问题，存在全局最优解)，用正规方程normal equation能直接得出答案，即直接求解导数为0的值下式：$$\frac{\partial}{\partial \theta_j} J(\theta_j) = 0$$ 对于训练集特征矩阵为$\boldsymbol{X}$（包含了$\boldsymbol{x}_0=1$）， 得到的解向量为:$$\theta = (\boldsymbol{X}^{\mathrm{\top}} \times \boldsymbol{X})^{-1} \times \boldsymbol{X}^{\mathrm{\top}} \times \boldsymbol{y}$$ 对于不可逆矩阵(如feature间相关，或者特征数大于训练集数等)，正规方程不能用。 梯度下降与正规方程的比较：]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>linear-regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[3 多变量线性回归]]></title>
    <url>%2F2017%2F08%2F30%2FlinearRegressionWithMultiVariable%2F</url>
    <content type="text"><![CDATA[实际应用场景中一般都是多个features特征，例如房间数，楼层等，构成一个多变量的模型，模型中的特征为$（x_1,x_2,…,x_n）$： $n$ 特征的数量 $\boldsymbol{x}^{(i)}$ 第$i$个训练实例，是特征矩阵中的第$i$行，是一个向量（vector）。 $x^{(i)}_j $ 第$i$个训练实例的第$j$个特征，即特征矩阵中第$i$行的第$j$个特征。 多变量线性回归 支持多变量的假设$h$ 为：$$h_{\theta}(x) = \theta_0 + \theta_1 x_1 \theta_2 x_2 + … + \theta_n x_n$$ 有 $n+1$ 个参数和$ n$ 个变量，为了方便处理，引入$x_0 = 1$，公式转换为: $$h_{\theta}(x) = \theta_0 x_0 + \theta_1 x_1 \theta_2 x_2 + … + \theta_n x_n$$ 这样，参数 $n+1$ 维的向量，训练实例也是一个$n+1$ 维向量，我们就可以直接用矩阵操作了。 特征矩阵(训练实例) 维度是 $m×(n+1)$, 公式也就简化为： $$h_{\theta} (\boldsymbol{X}) = \theta ^\mathrm{\top} \boldsymbol{X}$$ 多变量梯度下降gradient Descent for multiple variables与单变量梯度下降相似，构造损失函数： $$J(\theta_0 , \theta_1 ,…,\theta_n) = \frac{1}{2m} \sum^m_{i=1}(h_{\theta} (x^{(i)}) - y^{(i)})^2$$ 对$\theta_i$的参数求导，批量梯度下降，直到收敛： 求导后，代入： 计算过程是先随机选取一系列参数值，然后计算预测结果，代入上式，更新参数值，直到参数收敛。这里会遇到一些问题，一个是不同参数的尺度问题，还有的学习率问题，下面的2个tricks可以帮助解决这些问题： 特征缩放 Feature Scaling以上面的房价预测为例，房间数为0-5，尺寸是0-2000平方英尺，这样画出来的等高线是一个很长的椭圆，需要很多次迭代才能收敛： 为了使收敛速度更快（另外不同的数据级在其他的算法中可能需要一起操作，大级别的参数会覆盖小级别参数的变化），我们要保证这些特征具有相似的尺度similar scale，可以帮助梯度更快的收敛： 解决的方法是尝试将参数的范围放大[-1, 1]间，最简单的方法是：$$x_n=\frac{x_n - \mu_n}{S_n}$$ 其中 $\mu_n$ 是平均值，$S_n$是 标准差。 简单的说就是这个：$$x^*_i=\frac{x_i - \bar{x}}{x_{max} - x_{min}}$$ 特征缩放也称为： Normalization 归一化 学习率learning rate 收敛所需要的迭代次数根据模型不同而不同，可以用迭代次数与代价函数的图标来感官的查看： 代价函数应该在每次迭代后都应该有所减少，直到收敛，上图最后的浅蓝部分就收敛的不错了。 学习率过小，迭代次数太多，学习率太大，可能会错过局部最小值，通常考虑下面的学习率： 0.01，0.03，0.1，0.3，1，3， 10]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>linear-regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2 单变量线性回归]]></title>
    <url>%2F2017%2F08%2F30%2FlinearRegressionWithOneVariable%2F</url>
    <content type="text"><![CDATA[单变量线性回归 linear regression with one variable房价预测： 问题描述： $m $ 代表训练集中实例的数量 $x $ 代表特征/输入变量 $y $ 代表目标变量/输出变量 $(\boldsymbol{x,y}) $ 代表训练集中的实例 $ (x^{(i)},y^{(i)}) $ 代表第i个观察实例 $h $ 代表学习算法的解决方案或函数也称为假设（hypothesis）: 由训练集通过学习算法，学习得到一个假设$h$（这个假设就是一个模型），新的数据通过h，预测出房价。 如何表达$h$？这个例子中，我们先用一个线性方程：$$h_{\theta}=\theta_0 + \theta_1 x$$ 因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题. 线性关系（Linearity）是变量与自变量成一次方的函数关系。其画在函数图上会呈现一条直线。 代价函数cost function 确定了$h$的形式，我们就要为模型选择合适的参数parameters ： $\theta_0, \theta_1$。 预测值与实际值的差距就是建模误差modeling error 我们选择的建模误差的平方和( square error function)作为模型的代价函数，目标就是选择使得代价函数最小的模型参数。 总结 Hypothesis： $h_{\theta}=\theta_0 + \theta_1 x$ Parameters: $\theta_0, \theta_1$ Cost function: 其中$m$为样本个数 Goal : 最小化代价函数 下面的等高图，三个坐标分别为 $\theta_0, \theta_1, J(\theta_0, \theta_1)$：我们就是要找到那个凹点，使得$ J(\theta_0, \theta_1)$值最小。 如何找？下面介绍一种方法来求解代价函数的最小值。 梯度下降Gradient descent 思想：初始化参数（随机找一个参数组合 $\theta_0, \theta_1, … \theta_n$, 目标是寻找下一个能让代价函数值下降最多的参数组合，持续这么做直到一个局部最小值（local minimum）。 因为没有试过所有的参数组合，所以不能确认局部最小值是否是全局最小值（global minimum） 选择不同的初始参数组合，可能会找到不同的局部最小值。下图显示使用不同的初始值到达的不同的local minimum： Gradient descent algorithm 沿下降最快的方向（导数）梯度下降(参数必须同步更新)，下降率由learning Rate控制： 下降速度最快的方向为什么是导数方向：直观的例子，在一个下坡路，找一条离坡地最近的路：当然是沿着坡道直下（导数方向）而不是走Z字行下：上坡为省力选择Z字小坡度上，但是距离也更长；上图也可以想象为一个山脉，你想找到一条离山谷最近的路，当然是走最陡峭的路最近（这是一种贪婪算法，不一定是最优），最陡峭也就是导数方向(曲面的导数）。 为什么是减去？下图分为了左右两部分下降：左边部分因为导数是负数，减的话就成加了： 这样每个参数都是在它的导数即下降最快的方向下降。 学习率 α 太小，学习太慢 太大，可能错过最优点：fail to converge or even diverge 即使固定α，因为随着导数变小，后面的step也会逐渐变小的，所以也能够converge： 收敛的标准 最好当然是变化为0 $J(\boldsymbol{\theta}) 变化不超过过1/1000,我们就认为converge了 批量梯度下降（batch gradient descent） batch： 每次都用到所有的training data对于前面的线性回归问题，运用batch gradient descent方法，关键是求出代价函数的导数，首先回顾一下模型： 计算两个参数的倒数： 这样我们将梯度下降公式做一下替换： References 吴恩达，《Machine learning》，Coursera.]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>gradient-descent</tag>
        <tag>linear-regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1 机器学习概述]]></title>
    <url>%2F2017%2F08%2F30%2FmachineLearning%2F</url>
    <content type="text"><![CDATA[机器学习: 从数据中学到知识。 Computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P improves with experience E. from Tom Mitchell,1998. T: 任务； P: performance measure E: experience经验，实验 基于E的不断学习，通过P的指标得出在T上的表现得到改进，我们就说程序通过经验E来学习该任务。 如预计房价： 最简单的方法就是用一根直线(也就是一次方程)来预测，用二次函数能够得到更好的结果： 这种学习称为 监督学习 supervised learning: 预先有正确的结果，可以用来指导，检验算法的正确性。 监督学习 supervised learning 1 回归regression ： 连续值预测 2 分类classification： 离散值预测，如何根据特征将对象分到不同的分类中(赋予不同的离散值) - 上面的例子只用到两个特征 features，更多的特征处理可能需要其他算法，如SVM。 非监督学习 unsupervised learning 特点：训练数据只有特征，没有结果：发现这些数据是否可以分为不同的组,应用包括： 聚类问题Clustering 声音辨别：将不同人的声音分离，使用双麦克，位置的远近，角度会造成声音的大小等不同。使用svd一行代码就能解决。 References 吴恩达，《Machine learning》，Coursera.]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为什么梯度反方向下降最快？]]></title>
    <url>%2F2017%2F08%2F20%2FwhyDescent%2F</url>
    <content type="text"><![CDATA[梯度下降算法中，为什么沿着梯度反方向是下降最快呢？用一种直观的方法”证明”一下。考虑两个features的情况。首先用泰勒一阶展开，问题转换为在点(a,b)附近（我们可以限制在以点为圆心的圆内）哪个方向可以使得函数值最小？ 转换成一个更好理解的公式：看一下公式的后两项，就是两个向量的点积，$[u,v]^ \top$是已知的，要使点积值最小（这里负值是最小的了），当然是和$[u,v]^ \top$方向相反，并且尽可能远的向量了： 参考 李宏毅，《machine learning》]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值计算需要注意的问题]]></title>
    <url>%2F2017%2F08%2F19%2FnumCalculate01%2F</url>
    <content type="text"><![CDATA[数值计算中，需要考虑计算机精度带来的一些问题。 Overflow and Underflow上溢和下溢数字计算机无法对实数进行”精确”表示,总会引入些误差（大部分是舍入误差rounding errors）。当这些误差被复合累积操作时，会造成一些问题。 误差的下溢 ： 接近零的的数被四舍五入为零。很多时候等于零与接近零的表现质的不同。上溢：当大量级的数被近似为$\infty, -\infty$,进一步计算会导致非数字。例如softmax函数，经常用于预测与multinoulli 分布相关联的概率，定义为： 考虑所有$x$都等于$c$，那么softmax的理论值为$\frac{1}{n}$。但是在数值计算中，当$c$量级很大时，可能求不到： If $c$ is very negative, $\exp (c)$会underflow：造成分母为0。 $c$是一个很大数，造成$\exp (c)$overflow； 我们可以通过计算$\mathrm{softmax} (\boldsymbol{z})$来避免这些问题，其中 $\boldsymbol{z = x} - \max _i x_i$ : 减去$\max _i x_i$导致exp最大参数为0；不会overflow； 同样的原因，分布至少有一个值为1，排除分母underflow变成被0除的可能。 另外分子的underflow可导致整体结果为零。若在计算$log (softmax(x))$ 函数中，先计算softmax，再代入计算log有可能出现负无穷结果。所以我们要实现一个单独的函数以数值稳定的方式来计算。 所以计算时一定要考虑数值不稳定的问题，必须设计为最小化舍入误差的积累。 病态条件数Poor Conditioning条件数conditioning是指函数相对于输入的微小变化而变化的快慢程度，而迅速变化的函数是不理想的：舍入误差会造成很大的变化。 考虑函数$f(x) = A^{-1} x$, 当$A \in \mathbb{R}^{n \times n}$具有特征分解时，其条件数为：$$\underset{i,j}{\max}~ \Bigg| \frac{\lambda_i}{ \lambda_j} \Bigg|.$$这是最大和最小特征值的模之比,当该数很大时，矩阵求逆对输入的误差特别敏感。注意这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结果。 即使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存在的误差。 在实践中，该错误将与求逆过程本身的数值误差进一步复合。]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>数值计算</tag>
        <tag>overflow &amp; underflow</tag>
        <tag>poor conditioning</tag>
      </tags>
  </entry>
</search>
