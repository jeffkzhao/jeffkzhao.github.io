<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[3 多变量线性回归]]></title>
    <url>%2F2017%2F08%2F30%2FlinearRegressionWithMultiVariable%2F</url>
    <content type="text"><![CDATA[实际应用场景中一般都是多个features特征，例如房间数，楼层等，构成一个多变量的模型，模型中的特征为$（x_1,x_2,…,x_n）$： $n$ 特征的数量 $\boldsymbol{x}^{(i)}$ 第$i$个训练实例，是特征矩阵中的第$i$行，是一个向量（vector）。 $x^{(i)}_j $ 第$i$个训练实例的第$j$个特征，即特征矩阵中第$i$行的第$j$个特征。 多变量线性回归 支持多变量的假设$h$ 为：$$h_{\theta}(x) = \theta_0 + \theta_1 x_1 \theta_2 x_2 + … + \theta_n x_n$$ 有 $n+1$ 个参数和$ n$ 个变量，为了方便处理，引入$x_0 = 1$，公式转换为: $$h_{\theta}(x) = \theta_0 x_0 + \theta_1 x_1 \theta_2 x_2 + … + \theta_n x_n$$ 这样，参数 $n+1$ 维的向量，训练实例也是一个$n+1$ 维向量，我们就可以直接用矩阵操作了。 特征矩阵(训练实例) 维度是 $m×(n+1)$, 公式也就简化为： $$h_{\theta} (\boldsymbol{X}) = \theta ^\mathrm{\top} \boldsymbol{X}$$ 多变量梯度下降gradient Descent for multiple variables与单变量梯度下降相似，构造损失函数： $$J(\theta_0 , \theta_1 ,…,\theta_n) = \frac{1}{2m} \sum^m_{i=1}(h_{\theta} (x^{(i)}) - y^{(i)})^2$$ 对$\theta_i$的参数求导，批量梯度下降，直到收敛： 求导后，代入： 计算过程是先随机选取一系列参数值，然后计算预测结果，代入上式，更新参数值，直到参数收敛。这里会遇到一些问题，一个是不同参数的尺度问题，还有的学习率问题，下面的2个tricks可以帮助解决这些问题： 特征缩放 Feature Scaling以上面的房价预测为例，房间数为0-5，尺寸是0-2000平方英尺，这样画出来的等高线是一个很长的椭圆，需要很多次迭代才能收敛： 为了使收敛速度更快（另外不同的数据级在其他的算法中可能需要一起操作，大级别的参数会覆盖小级别参数的变化），我们要保证这些特征具有相似的尺度similar scale，可以帮助梯度更快的收敛： 解决的方法是尝试将参数的范围放大[-1, 1]间，最简单的方法是：$$x_n=\frac{x_n - \mu_n}{S_n}$$ 其中 $\mu_n$ 是平均值，$S_n$是 标准差。 简单的说就是这个：$$x^*_i=\frac{x_i - \bar{x}}{x_{max} - x_{min}}$$ 特征缩放也称为： Normalization 归一化 学习率learning rate 收敛所需要的迭代次数根据模型不同而不同，可以用迭代次数与代价函数的图标来感官的查看： 代价函数应该在每次迭代后都应该有所减少，直到收敛，上图最后的浅蓝部分就收敛的不错了。 学习率过小，迭代次数太多，学习率太大，可能会错过局部最小值，通常考虑下面的学习率： 0.01，0.03，0.1，0.3，1，3， 10]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>linear-regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2 单变量线性回归]]></title>
    <url>%2F2017%2F08%2F30%2FlinearRegressionWithOneVariable%2F</url>
    <content type="text"><![CDATA[单变量线性回归 linear regression with one variable房价预测： 问题描述： $m $ 代表训练集中实例的数量 $x $ 代表特征/输入变量 $y $ 代表目标变量/输出变量 $(\boldsymbol{x,y}) $ 代表训练集中的实例 $ (x^{(i)},y^{(i)}) $ 代表第i个观察实例 $h $ 代表学习算法的解决方案或函数也称为假设（hypothesis）: 由训练集通过学习算法，学习得到一个假设$h$（这个假设就是一个模型），新的数据通过h，预测出房价。 如何表达$h$？这个例子中，我们先用一个线性方程：$$h_{\theta}=\theta_0 + \theta_1 x$$ 因为只含有一个特征/输入变量，因此这样的问题叫作单变量线性回归问题. 线性关系（Linearity）是变量与自变量成一次方的函数关系。其画在函数图上会呈现一条直线。 代价函数cost function 确定了$h$的形式，我们就要为模型选择合适的参数parameters ： $\theta_0, \theta_1$。 预测值与实际值的差距就是建模误差modeling error 我们选择的建模误差的平方和( square error function)作为模型的代价函数，目标就是选择使得代价函数最小的模型参数。 总结 Hypothesis： $h_{\theta}=\theta_0 + \theta_1 x$ Parameters: $\theta_0, \theta_1$ Cost function: 其中$m$为样本个数 Goal : 最小化代价函数 下面的等高图，三个坐标分别为 $\theta_0, \theta_1, J(\theta_0, \theta_1)$：我们就是要找到那个凹点，使得$ J(\theta_0, \theta_1)$值最小。 如何找？下面介绍一种方法来求解代价函数的最小值。 梯度下降Gradient descent 思想：初始化参数（随机找一个参数组合 $\theta_0, \theta_1, … \theta_n$, 目标是寻找下一个能让代价函数值下降最多的参数组合，持续这么做直到一个局部最小值（local minimum）。 因为没有试过所有的参数组合，所以不能确认局部最小值是否是全局最小值（global minimum） 选择不同的初始参数组合，可能会找到不同的局部最小值。下图显示使用不同的初始值到达的不同的local minimum： Gradient descent algorithm 沿下降最快的方向（导数）梯度下降(参数必须同步更新)，下降率由learning Rate控制： 下降速度最快的方向为什么是导数方向：直观的例子，在一个下坡路，找一条离坡地最近的路：当然是沿着坡道直下（导数方向）而不是走Z字行下：上坡为省力选择Z字小坡度上，但是距离也更长；上图也可以想象为一个山脉，你想找到一条离山谷最近的路，当然是走最陡峭的路最近（这是一种贪婪算法，不一定是最优），最陡峭也就是导数方向(曲面的导数）。 为什么是减去？下图分为了左右两部分下降：左边部分因为导数是负数，减的话就成加了： 这样每个参数都是在它的导数即下降最快的方向下降。 学习率 α 太小，学习太慢 太大，可能错过最优点：fail to converge or even diverge 即使固定α，因为随着导数变小，后面的step也会逐渐变小的，所以也能够converge： 收敛的标准 最好当然是变化为0 $J(\boldsymbol{\theta}) 变化不超过过1/1000,我们就认为converge了 批量梯度下降（batch gradient descent） batch： 每次都用到所有的training data对于前面的线性回归问题，运用batch gradient descent方法，关键是求出代价函数的导数，首先回顾一下模型： 计算两个参数的倒数： 这样我们将梯度下降公式做一下替换： References 吴恩达，《Machine learning》，Coursera.]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
      <tags>
        <tag>machine-learning</tag>
        <tag>linear-regression</tag>
        <tag>gradient-descent</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[1 机器学习概述]]></title>
    <url>%2F2017%2F08%2F30%2FmachineLearning%2F</url>
    <content type="text"><![CDATA[机器学习: 从数据中学到知识。 Computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P improves with experience E. from Tom Mitchell,1998. T: 任务； P: performance measure E: experience经验，实验 基于E的不断学习，通过P的指标得出在T上的表现得到改进，我们就说程序通过经验E来学习该任务。 如预计房价： 最简单的方法就是用一根直线(也就是一次方程)来预测，用二次函数能够得到更好的结果： 这种学习称为 监督学习 supervised learning: 预先有正确的结果，可以用来指导，检验算法的正确性。 监督学习 supervised learning 1 回归regression ： 连续值预测 2 分类classification： 离散值预测，如何根据特征将对象分到不同的分类中(赋予不同的离散值) - 上面的例子只用到两个特征 features，更多的特征处理可能需要其他算法，如SVM。 非监督学习 unsupervised learning 特点：训练数据只有特征，没有结果：发现这些数据是否可以分为不同的组,应用包括： 聚类问题Clustering 声音辨别：将不同人的声音分离，使用双麦克，位置的远近，角度会造成声音的大小等不同。使用svd一行代码就能解决。 References 吴恩达，《Machine learning》，Coursera.]]></content>
      <categories>
        <category>吴恩达Coursera笔记</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[为什么梯度反方向下降最快？]]></title>
    <url>%2F2017%2F08%2F20%2FwhyDescent%2F</url>
    <content type="text"><![CDATA[梯度下降算法中，为什么沿着梯度反方向是下降最快呢？用一种直观的方法”证明”一下。考虑两个features的情况。首先用泰勒一阶展开，问题转换为在点(a,b)附近（我们可以限制在以点为圆心的圆内）哪个方向可以使得函数值最小？ 转换成一个更好理解的公式：看一下公式的后两项，就是两个向量的点积，$[u,v]^ \top$是已知的，要使点积值最小（这里负值是最小的了），当然是和$[u,v]^ \top$方向相反，并且尽可能远的向量了： 参考 李宏毅，《machine learning》]]></content>
      <categories>
        <category>machine-learning</category>
      </categories>
      <tags>
        <tag>梯度下降</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数值计算需要注意的问题]]></title>
    <url>%2F2017%2F08%2F19%2FnumCalculate01%2F</url>
    <content type="text"><![CDATA[数值计算中，需要考虑计算机精度带来的一些问题。 Overflow and Underflow上溢和下溢数字计算机无法对实数进行”精确”表示,总会引入些误差（大部分是舍入误差rounding errors）。当这些误差被复合累积操作时，会造成一些问题。 误差的下溢 ： 接近零的的数被四舍五入为零。很多时候等于零与接近零的表现质的不同。上溢：当大量级的数被近似为$\infty, -\infty$,进一步计算会导致非数字。例如softmax函数，经常用于预测与multinoulli 分布相关联的概率，定义为： 考虑所有$x$都等于$c$，那么softmax的理论值为$\frac{1}{n}$。但是在数值计算中，当$c$量级很大时，可能求不到： If $c$ is very negative, $\exp (c)$会underflow：造成分母为0。 $c$是一个很大数，造成$\exp (c)$overflow； 我们可以通过计算$\mathrm{softmax} (\boldsymbol{z})$来避免这些问题，其中 $\boldsymbol{z = x} - \max _i x_i$ : 减去$\max _i x_i$导致exp最大参数为0；不会overflow； 同样的原因，分布至少有一个值为1，排除分母underflow变成被0除的可能。 另外分子的underflow可导致整体结果为零。若在计算$log (softmax(x))$ 函数中，先计算softmax，再代入计算log有可能出现负无穷结果。所以我们要实现一个单独的函数以数值稳定的方式来计算。 所以计算时一定要考虑数值不稳定的问题，必须设计为最小化舍入误差的积累。 病态条件数Poor Conditioning条件数conditioning是指函数相对于输入的微小变化而变化的快慢程度，而迅速变化的函数是不理想的：舍入误差会造成很大的变化。 考虑函数$f(x) = A^{-1} x$, 当$A \in \mathbb{R}^{n \times n}$具有特征分解时，其条件数为：$$\underset{i,j}{\max}~ \Bigg| \frac{\lambda_i}{ \lambda_j} \Bigg|.$$这是最大和最小特征值的模之比,当该数很大时，矩阵求逆对输入的误差特别敏感。注意这种敏感性是矩阵本身的固有特性，而不是矩阵求逆期间舍入误差的结果。 即使我们乘以完全正确的矩阵逆，病态条件的矩阵也会放大预先存在的误差。 在实践中，该错误将与求逆过程本身的数值误差进一步复合。]]></content>
      <categories>
        <category>math</category>
      </categories>
      <tags>
        <tag>数值计算</tag>
        <tag>overflow &amp; underflow</tag>
        <tag>poor conditioning</tag>
      </tags>
  </entry>
</search>
